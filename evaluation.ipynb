{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from scipy.special import rel_entr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Assignment 2: Evaluation\n",
    "\n",
    "This notebook implements the evaluation metrics discussed in the second part of the tutorial and looks at some example data. First we need a function that calculates the Jensen-Shannon divergence and cross-entropy between two probability distributions."
   ],
   "id": "44a54e39916f1713"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def jenson_shannon_divergence(human_dist, predicted_dist):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon divergence between two probability distributions.\n",
    "    :param human_dist: a probability distribution\n",
    "    :param predicted_dist: a probability distribution\n",
    "    :return: the Jensen-Shannon divergence between the two distributions\n",
    "    \"\"\"\n",
    "    # Calculate the mid distribution\n",
    "    average_distribution = 0.5 * (human_dist + predicted_dist)\n",
    "    # kl-div between human and average\n",
    "    kldiv_a = sum(rel_entr(human_dist, average_distribution))\n",
    "    # kl-div between predicted and average\n",
    "    kldiv_b = sum(rel_entr(predicted_dist, average_distribution))\n",
    "    # Calculate the Jensen-Shannon divergence\n",
    "    divergence = 0.5 * (kldiv_a + kldiv_b)\n",
    "\n",
    "    return divergence"
   ],
   "id": "45ac084bfe23ba74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_entropy(human_dist, predicted_dist):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy between two probability distributions.\n",
    "    :param human_dist: a probability distribution\n",
    "    :param predicted_dist: a probability distribution\n",
    "    :return: the cross-entropy between the two distributions\n",
    "    \"\"\"\n",
    "    # Calculate the cross-entropy\n",
    "    ce = -sum(rel_entr(human_dist, predicted_dist))\n",
    "\n",
    "    return ce"
   ],
   "id": "40bae65a8245c773",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we can calculate the Jensen-Shannon divergence and cross-entropy between the human and predicted distributions for differenet scenarios, as defined below.",
   "id": "455f8516b7d5f7ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# some example distributions for human data\n",
    "human_distribution = {\n",
    "\"humans low uncertainty\" : [0.9, 0.05, 0.05],\n",
    "\"humans moderate uncertainty\" : [0.5, 0.3, 0.2],\n",
    "\"humans high uncertainty\": [0.33, 0.33, 0.33],\n",
    "\"humans medium uncertainty - one dominant\":  [0.7, 0.2, 0.1],\n",
    "\"humans low uncertainty - one dominant\": [0.8, 0.1, 0.1],\n",
    "}\n",
    "\n",
    "predicted_distributions = {\n",
    "\"prediction high certainty wrong class\": [0.05, 0.9, 0.05],\n",
    "\"prediction high certainty correct class\": [0.9, 0.05, 0.05],\n",
    "\"prediction moderate certainty wrong class\": [0.4, 0.5, 0.1],\n",
    "\"prediction max uncertainty\": [0.33, 0.33, 0.33],\n",
    "\"prediction moderate certainty correct class\": [0.5, 0.3, 0.2],\n",
    "    \n",
    "}\n"
   ],
   "id": "1c53bf47e288ad07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d0e32e5248ebfab4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A1: Calculate the Jensen-Shannon divergence and cross-entropy between the human and predicted distributions for all possible cases. Which cases lead to a very low JS-divergence and cross-entropy? What does this mean? Which cases lead to a very high JS-divergence and cross-entropy? What does this mean? In which cases do JS-divergence and cross-entropy differ significantly? ",
   "id": "35fadb8d2764db8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we look at example data and example model predictions. We want to use different metrics to evaluate to what extent the model predictions agree with the human annotations.",
   "id": "b16970866e77262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_f1_per_annotator(annotations, model_predictions):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for each annotator.\n",
    "    :param annotations: a dataframe with the annotations for each annotator.\n",
    "    :param model_predictions: a vector with the model predictions.\n",
    "    :return: a list with the F1 score for each annotator. The list has length equal to the number of annotators.\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    for annotator in annotations.columns:\n",
    "        f1_scores.append(f1_score(annotations[annotator], model_predictions > 0.5))\n",
    "    return f1_scores"
   ],
   "id": "fd27d70c7e4dd27a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def normalized_entropy(prob_dist):\n",
    "    \"\"\"\n",
    "    Calculate the normalized entropy of a probability distribution.\n",
    "    :param prob_dist: a probability distribution, e.g. a human distribution over 2 or more classes.\n",
    "    :return: normalized entropy of the probability distribution, value between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Calculate entropy with base 2 (in bits)\n",
    "    ent = entropy(prob_dist, base=2)\n",
    "    # Max entropy occurs when all categories are equally likely\n",
    "    max_ent = np.log2(len(prob_dist))\n",
    "    # Normalize the entropy (divide by max possible entropy) take into account that entropy is 0 when all are the same\n",
    "    if max_ent == 0:\n",
    "        norm_entropy = 0\n",
    "    else:\n",
    "        norm_entropy = ent / max_ent\n",
    "    return norm_entropy"
   ],
   "id": "70d87a2d1dbe54aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def entropy_similarity(human_entropy_scores, model_entropy_scores):\n",
    "    \"\"\"\n",
    "    Calculate the entropy similarity between two probability distributions.\n",
    "    :param human_entropy_scores: a probability distribution\n",
    "    :param model_entropy_scores: a probability distribution\n",
    "    :return: the entropy similarity between the two distributions\n",
    "    \"\"\"\n",
    "    # Calculate the cosine similarity between these two vectors\n",
    "    ent_sim = np.dot(human_entropy_scores, model_entropy_scores) / (\n",
    "            np.linalg.norm(human_entropy_scores) * np.linalg.norm(model_entropy_scores)\n",
    "    )\n",
    "\n",
    "    return ent_sim"
   ],
   "id": "15244ed1cb194eac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_normalized_human_distribution(annotated_labels, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate the normalized human distribution.\n",
    "    :param annotated_labels: the annotated labels\n",
    "    :param num_classes: the number of classes\n",
    "    :return: the normalized human distribution\n",
    "    \"\"\"\n",
    "    # Calculate the histogram of the annotated labels\n",
    "    hist, _ = np.histogram(annotated_labels, bins=num_classes, range=(0, num_classes))\n",
    "    # Normalize the histogram\n",
    "    normalized_human_distribution = hist / len(annotated_labels)\n",
    "\n",
    "    return normalized_human_distribution"
   ],
   "id": "f028ccbb49d987c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def entropy_correlation(human_entropy_scores, model_entropy_scores):\n",
    "    \"\"\"\n",
    "    Calculate the entropy correlation between two probability distributions.\n",
    "    :param human_entropy_scores: a probability distribution\n",
    "    :param model_entropy_scores: a probability distribution\n",
    "    :return: the entropy correlation between the two distributions\n",
    "    \"\"\"\n",
    "    # Calculate the pearson correlation between these two vectors\n",
    "    corr = np.corrcoef(human_entropy_scores, model_entropy_scores)[0, 1]\n",
    "\n",
    "    return corr"
   ],
   "id": "b493c5cb24ed2f05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can simulate some example data and model predictions and calculate different metrics for both datasets, both models (Model A and Model B).",
   "id": "d38ca31dfa902581"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# 100 samples with 3 annotators providing binary labels (0 or 1)\n",
    "annotations_3_annotators = pd.DataFrame({\n",
    "    'Annotator_1': np.random.randint(0, 2, 100),\n",
    "    'Annotator_2': np.random.randint(0, 2, 100),\n",
    "    'Annotator_3': np.random.randint(0, 2, 100)\n",
    "})\n",
    "annotations_3_annotators['human_distribution'] = [get_normalized_human_distribution(annotations_3_annotators.iloc[i], 2) for i in range(annotations_3_annotators.shape[0])]\n",
    "# Simulate predictions by two models (Model A and Model B) for 3 annotators\n",
    "model_A_predictions_3 = np.random.uniform(0.3, 0.7, 100)  \n",
    "model_B_predictions_3 = np.random.uniform(0.2, 0.8, 100)  \n",
    "annotations_3_annotators[\"MODEL_A\"] = model_A_predictions_3\n",
    "annotations_3_annotators[\"MODEL_B\"] = model_B_predictions_3\n",
    "# convert them into a probability distribution by putting the probability into index 1, if it is > 0.5 and into index 0 if it is <= 0.5, and add 1- probability to the other index\n",
    "annotations_3_annotators[\"MODEL_A\"] = annotations_3_annotators[\"MODEL_A\"].apply(lambda x: [1-x, x] if x <= 0.5 else [x, 1-x])\n",
    "annotations_3_annotators[\"MODEL_B\"] = annotations_3_annotators[\"MODEL_B\"].apply(lambda x: [1-x, x] if x <= 0.5 else [x, 1-x])\n",
    "\n",
    "# Simulate the dataset for 5 annotators\n",
    "annotations_5_annotators = pd.DataFrame({\n",
    "    'Annotator_1': np.random.randint(0, 2, 100),\n",
    "    'Annotator_2': np.random.randint(0, 2, 100),\n",
    "    'Annotator_3': np.random.randint(0, 2, 100),\n",
    "    'Annotator_4': np.random.randint(0, 2, 100),\n",
    "    'Annotator_5': np.random.randint(0, 2, 100)\n",
    "})\n",
    "annotations_5_annotators['human_distribution'] = [get_normalized_human_distribution(annotations_5_annotators.iloc[i], 2) for i in range(annotations_5_annotators.shape[0])]\n",
    "\n",
    "# Simulate predictions by two models (Model A and Model B) for 5 annotators\n",
    "model_A_predictions_5 = np.random.uniform(0.3, 0.7, 100)  \n",
    "model_B_predictions_5 = np.random.uniform(0.2, 0.8, 100)  \n",
    "annotations_5_annotators[\"MODEL_A\"] = model_A_predictions_5\n",
    "annotations_5_annotators[\"MODEL_B\"] = model_B_predictions_5\n",
    "annotations_5_annotators[\"MODEL_A\"] = annotations_5_annotators[\"MODEL_A\"].apply(lambda x: [1-x, x] if x <= 0.5 else [x, 1-x])\n",
    "annotations_5_annotators[\"MODEL_B\"] = annotations_5_annotators[\"MODEL_B\"].apply(lambda x: [1-x, x] if x <= 0.5 else [x, 1-x])\n",
    "\n",
    "\n",
    "\n",
    "# to each dataset add the entropy of each item and the human distribution\n",
    "annotations_3_annotators['human entropy'] = annotations_3_annotators.human_distribution.apply(lambda x: normalized_entropy(x))\n",
    "\n",
    "annotations_5_annotators['human entropy'] = annotations_5_annotators.human_distribution.apply(lambda x: normalized_entropy(x))\n",
    "annotations_3_annotators['model_A_entropy'] = annotations_3_annotators.MODEL_A.apply(lambda x: normalized_entropy(x))\n",
    "annotations_3_annotators['model_B_entropy'] = annotations_3_annotators.MODEL_B.apply(lambda x: normalized_entropy(x))\n",
    "print(tabulate(annotations_3_annotators.head(), headers='keys', tablefmt='pretty'))"
   ],
   "id": "faf3f722a609831a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d8dc2f0bfd48827c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataframes should contain all information that are needed to calculate different metrics. Calculate some metrics of your choice for the datasets and models and compare the performance of the two models. What do you observe?",
   "id": "f3747a31bf30cd3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
