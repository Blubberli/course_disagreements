{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from krippendorff import alpha\n",
    "from pingouin import intraclass_corr\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Assignment 1: Annotation\n",
    "\n",
    "This notebook implements the annotation metrics discussed in the first part of the tutorial and looks at some example data."
   ],
   "id": "44a54e39916f1713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us create the example data we looked at in the tutorial. We have one dataset with two annotators who annotated three categories (three emotions). We have another dataset with three annotators who annotated the same three categories. Finally, we use the dataset with three annotators but treat the categories as continuous values, as if they are rated for, for instance something like emotion intensity on a scale from 1 to 3. We can check if the values are in line with the values on the slides.",
   "id": "de771db9739f9bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "a1 = [0, 1, 1, 2, 1]\n",
    "a2 = [0, 2, 1, 2, 2]\n",
    "a3 = [0, 1, 1, 1, 2]\n",
    "\n",
    "# first compute the cohens kappa between a1 and a2\n",
    "kappa = cohen_kappa_score(a1, a2)\n",
    "print(f\"Cohen's Kappa between a1 and a2: {kappa}\")\n",
    "\n",
    "# second, we compute the krippendorff's alpha between a1, a2, and a3\n",
    "alpha = alpha([a1, a2, a3], level_of_measurement='nominal')\n",
    "print(f\"Krippendorff's Alpha between a1, a2, and a3: {alpha}\")\n",
    "\n",
    "# finally, we compute the ICC2k between a1, a2, and a3\n",
    "# first we need to reshape the data, such that we have a dataframe with three columns: annotators, items and ratings\n",
    "data = pd.DataFrame({'annotator': ['a1']*5 + ['a2']*5 + ['a3']*5,\n",
    "                     'item': [1, 2, 3, 4, 5]*3,\n",
    "                     'rating': a1 + a2 + a3})\n",
    "icc = intraclass_corr(data, targets='item', raters='annotator', ratings='rating', nan_policy='omit')\n",
    "print(\"ICC values:\")\n",
    "print(tabulate(icc, headers='keys', tablefmt='fancy_grid'))"
   ],
   "id": "85dd810254c571b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Q1: What is the cohen's kappa between annotator a2 and a3?\n",
    "\n",
    "Q2: Why is the ICC value higher than Cohen's Kappa and Krippendorff's Alpha?"
   ],
   "id": "ee19a6696cc43f9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Consider the following example of annotated data. Two annotators had to classify an image into whether it contains a dog (0) or a cat(1). Look at the result for the percentage of agreement and Cohen's Kappa. ",
   "id": "bc66a6699db0a50c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generating data\n",
    "# Rater 1 has the skewed distribution: 95% cats, 5% dogs\n",
    "rater1 = np.array([1] * 95 + [0] * 5)\n",
    "\n",
    "# Rater 2 also rates based on a skewed perception\n",
    "deviation = np.random.rand(100) < 0.1  # 10% deviation\n",
    "rater2 = np.where(deviation, 1 - rater1, rater1)  # Flipping the label in case of a deviation\n",
    "\n",
    "# Calculate the percentage of agreement\n",
    "agreement_percentage = np.mean(rater1 == rater2) * 100\n",
    "\n",
    "kappa = cohen_kappa_score(rater1, rater2)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "data = pd.DataFrame({'Animal': ['Cat']*95 + ['Dog']*5, 'Rater 1': rater1, 'Rater 2': rater2})\n",
    "print(\"Sample of the ratings:\")\n",
    "print(data.sample(10))  # Show a random sample of the data\n",
    "\n",
    "print(f\"\\nPercentage of Agreement: {agreement_percentage:.2f}%\")\n",
    "print(f\"Cohen's Kappa Score: {kappa:.2f}\")\n",
    "\n",
    "# Simple analysis of category counts\n",
    "category_counts = data[['Rater 1', 'Rater 2']].apply(pd.Series.value_counts)\n",
    "print(\"\\nCategory counts per rater:\")\n",
    "print(category_counts)"
   ],
   "id": "eaf58291c7edbff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q3: Why is the percentage of agreement so high, even though the Cohen's Kappa is low?",
   "id": "16600665942b4435"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "high_agreement = [0.9, 0.05, 0.05]\n",
    "moderate_agreement = [0.5, 0.3, 0.2]\n",
    "equal_agreement = [0.33, 0.33, 0.33]\n",
    "one_dominant =  [0.7, 0.2, 0.1]\n",
    "one_majority = [0.8, 0.1, 0.1]"
   ],
   "id": "aeb70b484b2320dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalized_entropy(prob_dist):\n",
    "    # Calculate entropy with base 2 (in bits)\n",
    "    ent = entropy(prob_dist, base=2)\n",
    "    # Max entropy occurs when all categories are equally likely\n",
    "    max_ent = np.log2(len(prob_dist))\n",
    "    # Normalize the entropy (divide by max possible entropy) take into account that entropy is 0 when all are the same\n",
    "    if max_ent == 0:\n",
    "        norm_entropy = 0\n",
    "    else:\n",
    "        norm_entropy = ent / max_ent\n",
    "    return ent, norm_entropy"
   ],
   "id": "b39749dc9bb44515",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q4: Calculate the entropy and normalized entropy for the different distributions and sort them from low to high entropy. How is it related to the agreement between the raters?",
   "id": "1843fde1b6e56501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# High agreement: most raters agree\n",
    "data_high = pd.DataFrame({\n",
    "    'Item': [1, 2, 3, 4, 5],\n",
    "    'Rater 1': ['A', 'A', 'B', 'B', 'C'],\n",
    "    'Rater 2': ['A', 'C', 'B', 'B', 'C'],\n",
    "    'Rater 3': ['A', 'A', 'B', 'B', 'C']\n",
    "})\n",
    "\n",
    "# Moderate agreement: raters have some disagreement\n",
    "data_moderate = pd.DataFrame({\n",
    "    'Item': [1, 2, 3, 4, 5],\n",
    "    'Rater 1': ['A', 'B', 'B', 'A', 'C'],\n",
    "    'Rater 2': ['A', 'C', 'B', 'A', 'C'],\n",
    "    'Rater 3': ['A', 'B', 'A', 'A', 'B']\n",
    "})\n",
    "\n",
    "# Low agreement: raters have little agreement\n",
    "data_low = pd.DataFrame({\n",
    "    'Item': [1, 2, 3, 4, 5],\n",
    "    'Rater 1': ['A', 'C', 'B', 'A', 'C'],\n",
    "    'Rater 2': ['B', 'C', 'A', 'B', 'A'],\n",
    "    'Rater 3': ['C', 'A', 'C', 'A', 'B']\n",
    "})"
   ],
   "id": "2b26a1ec81c40590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_entropy_for_items(input_data):\n",
    "    items_entropy = []\n",
    "    for _, row in input_data.iterrows():\n",
    "        # Get the frequency distribution of categories for each item\n",
    "        counts = row[1:].value_counts(normalize=True)\n",
    "        ent, norm_ent = normalized_entropy(counts)\n",
    "        items_entropy.append({'Item': row['Item'], 'Normalized Entropy': norm_ent})\n",
    "    return pd.DataFrame(items_entropy)\n",
    "\n",
    "entropy_high = calculate_entropy_for_items(data_high)\n",
    "entropy_moderate = calculate_entropy_for_items(data_moderate)\n",
    "entropy_low = calculate_entropy_for_items(data_low)\n",
    "\n",
    "# Display Entropy Results for Each Scenario\n",
    "print(\"Entropy for High Agreement Scenario:\")\n",
    "print(entropy_high)\n",
    "print(\"\\nEntropy for Moderate Agreement Scenario:\")\n",
    "print(entropy_moderate)\n",
    "print(\"\\nEntropy for Low Agreement Scenario:\")\n",
    "print(entropy_low)"
   ],
   "id": "4292061f29abe6d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kappa_high = cohen_kappa_score(data_high['Rater 2'], data_high['Rater 3'])\n",
    "kappa_moderate = cohen_kappa_score(data_moderate['Rater 2'], data_moderate['Rater 3'])\n",
    "kappa_low = cohen_kappa_score(data_low['Rater 2'], data_low['Rater 3'])\n",
    "\n",
    "# Display Cohen's Kappa Results for Each Scenario\n",
    "print(\"\\nCohen's Kappa for High Agreement Scenario\")\n",
    "print(kappa_high)\n",
    "print(\"\\nCohen's Kappa for Moderate Agreement Scenario\")\n",
    "print(kappa_moderate)\n",
    "print(\"\\nCohen's Kappa for Low Agreement Scenario\")\n",
    "print(kappa_low)"
   ],
   "id": "5e7b43a00adf09f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plotting Normalized Entropy for Each Scenario\n",
    "scenarios = ['High Agreement', 'Moderate Agreement', 'Low Agreement']\n",
    "average_entropy = [\n",
    "    entropy_high['Normalized Entropy'].mean(),\n",
    "    entropy_moderate['Normalized Entropy'].mean(),\n",
    "    entropy_low['Normalized Entropy'].mean()\n",
    "]\n",
    "\n",
    "kappa_scenarios = [kappa_high, kappa_moderate, kappa_low]\n",
    "# create a bar plot that compares a bar for entropy and for kappa for each scenario. each color represents a different metric\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(scenarios))\n",
    "bar1 = ax.bar(index, average_entropy, bar_width, label='Entropy')\n",
    "bar2 = ax.bar(index + bar_width, kappa_scenarios, bar_width, label=\"Cohen's Kappa\")\n",
    "# add the labels of the scenarios to the x-axis\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(scenarios)\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Entropy and Cohen\\'s Kappa for Different Agreement Scenarios')\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "id": "c6964058a0622741",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q5: Compare the two metrics for the different scenarios using the plot. How do they differ in general between scenarios? How does kappa differ from entropy?",
   "id": "5a1c99f415d4847f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
